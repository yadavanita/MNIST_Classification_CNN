{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install useful libraries using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing.image\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import os\n",
    "import datetime  \n",
    "import cv2 \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm  \n",
    "%matplotlib inline\n",
    "\n",
    "#display parent directory and working directory\n",
    "print(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\n",
    "print(os.getcwd()+':', os.listdir(os.getcwd()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load and check data\n",
    "\n",
    "if os.path.isfile('data/train.csv'):\n",
    "    data_df = pd.read_csv('data/train.csv') # on local environment \n",
    "    print('train.csv loaded: data_df({0[0]},{0[1]})'.format(data_df.shape))\n",
    "else:\n",
    "    print('Error: train.csv not found')\n",
    "\n",
    "# basic info about data\n",
    "#print('')\n",
    "#print(data_df.info())\n",
    "\n",
    "# no missing values\n",
    "print('')\n",
    "print(data_df.isnull().any().describe())\n",
    "\n",
    "# 10 different labels ranging from 0 to 9\n",
    "print('')\n",
    "print('distinct labels ', data_df['label'].unique())\n",
    "\n",
    "# data are approximately balanced (less often occurs 5, most often 1)\n",
    "print('')\n",
    "print(data_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize data and split into training and validation sets\n",
    "\n",
    "# function to normalize data\n",
    "def normalize_data(data): \n",
    "    # scale features using statistics that are robust to outliers\n",
    "    data = data / data.max() # convert from [0:255] to [0.:1.]\n",
    "    return data\n",
    "\n",
    "# convert class labels from scalars to one-hot vectors e.g. 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "# convert one-hot encodings into labels\n",
    "def one_hot_to_dense(labels_one_hot):\n",
    "    return np.argmax(labels_one_hot,1)\n",
    "\n",
    "# computet the accuracy of label predictions\n",
    "def accuracy_from_dense_labels(y_target, y_pred):\n",
    "    y_target = y_target.reshape(-1,)\n",
    "    y_pred = y_pred.reshape(-1,)\n",
    "    return np.mean(y_target == y_pred)\n",
    "\n",
    "# computet the accuracy of one-hot encoded predictions\n",
    "def accuracy_from_one_hot_labels(y_target, y_pred):\n",
    "    y_target = one_hot_to_dense(y_target).reshape(-1,)\n",
    "    y_pred = one_hot_to_dense(y_pred).reshape(-1,)\n",
    "    return np.mean(y_target == y_pred)\n",
    "\n",
    "# extract and normalize images\n",
    "x_train_valid = data_df.iloc[:,1:].values.reshape(-1,28,28,1) # (42000,28,28,1) array\n",
    "x_train_valid = x_train_valid.astype(np.float) # convert from int64 to float32\n",
    "x_train_valid = normalize_data(x_train_valid)\n",
    "image_width = image_height = 28\n",
    "image_size = 784\n",
    "\n",
    "# extract image labels\n",
    "y_train_valid_labels = data_df.iloc[:,0].values # (42000,1) array\n",
    "labels_count = np.unique(y_train_valid_labels).shape[0]; # number of different labels = 10\n",
    "\n",
    "#plot some images and labels\n",
    "plt.figure(figsize=(15,9))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,1+i)\n",
    "    plt.title(y_train_valid_labels[i])\n",
    "    plt.imshow(x_train_valid[i].reshape(28,28), cmap=cm.binary)\n",
    "    \n",
    "# labels in one hot representation\n",
    "y_train_valid = dense_to_one_hot(y_train_valid_labels, labels_count).astype(np.uint8)\n",
    "\n",
    "# dictionaries for saving results\n",
    "y_valid_pred = {}\n",
    "y_train_pred = {}\n",
    "y_test_pred = {}\n",
    "train_loss, valid_loss = {}, {}\n",
    "train_acc, valid_acc = {}, {}\n",
    "\n",
    "print('x_train_valid.shape = ', x_train_valid.shape)\n",
    "print('y_train_valid_labels.shape = ', y_train_valid_labels.shape)\n",
    "print('image_size = ', image_size )\n",
    "print('image_width = ', image_width)\n",
    "print('image_height = ', image_height)\n",
    "print('labels_count = ', labels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## augment data\n",
    "\n",
    "# generate new images via rotations, translations, zoom using keras\n",
    "def generate_images(imgs):\n",
    "    \n",
    "    # rotations, translations, zoom\n",
    "    image_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n",
    "        zoom_range = 0.1)\n",
    "\n",
    "    # get transformed images\n",
    "    imgs = image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n",
    "                                batch_size=len(imgs), shuffle = False).next()    \n",
    "  \n",
    "    return imgs[0]\n",
    "\n",
    "# check image generation\n",
    "fig,axs = plt.subplots(5,10, figsize=(15,9))\n",
    "for i in range(5):\n",
    "    n = np.random.randint(0,x_train_valid.shape[0]-2)\n",
    "    axs[i,0].imshow(x_train_valid[n:n+1].reshape(28,28),cmap=cm.binary)\n",
    "    axs[i,1].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,2].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,3].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,4].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,5].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,6].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,7].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,8].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)\n",
    "    axs[i,9].imshow(generate_images(x_train_valid[n:n+1]).reshape(28,28), cmap=cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the neural network class\n",
    "\n",
    "class nn_class:\n",
    "# class that implements the neural network\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, nn_name = 'nn_1'):\n",
    "\n",
    "        # tunable hyperparameters for nn architecture\n",
    "        self.s_f_conv1 = 3; # filter size of first convolution layer (default = 3)\n",
    "        self.n_f_conv1 = 36; # number of features of first convolution layer (default = 36)\n",
    "        self.s_f_conv2 = 3; # filter size of second convolution layer (default = 3)\n",
    "        self.n_f_conv2 = 36; # number of features of second convolution layer (default = 36)\n",
    "        self.s_f_conv3 = 3; # filter size of third convolution layer (default = 3)\n",
    "        self.n_f_conv3 = 36; # number of features of third convolution layer (default = 36)\n",
    "        self.n_n_fc1 = 576; # number of neurons of first fully connected layer (default = 576)\n",
    "\n",
    "        # tunable hyperparameters for training\n",
    "        self.mb_size = 50 # mini batch size\n",
    "        self.keep_prob = 0.33 # keeping probability with dropout regularization \n",
    "        self.learn_rate_array = [10*1e-4, 7.5*1e-4, 5*1e-4, 2.5*1e-4, 1*1e-4, 1*1e-4,\n",
    "                                 1*1e-4,0.75*1e-4, 0.5*1e-4, 0.25*1e-4, 0.1*1e-4, \n",
    "                                 0.1*1e-4, 0.075*1e-4,0.050*1e-4, 0.025*1e-4, 0.01*1e-4, \n",
    "                                 0.0075*1e-4, 0.0050*1e-4,0.0025*1e-4,0.001*1e-4]\n",
    "        self.learn_rate_step_size = 3 # in terms of epochs\n",
    "        \n",
    "        # parameters\n",
    "        self.learn_rate = self.learn_rate_array[0]\n",
    "        self.learn_rate_pos = 0 # current position pointing to current learning rate\n",
    "        self.index_in_epoch = 0 \n",
    "        self.current_epoch = 0\n",
    "        self.log_step = 0.2 # log results in terms of epochs\n",
    "        self.n_log_step = 0 # counting current number of mini batches trained on\n",
    "        self.use_tb_summary = False # True = use tensorboard visualization\n",
    "        self.use_tf_saver = False # True = use saver to save the model\n",
    "        self.nn_name = nn_name # name of the neural network\n",
    "        \n",
    "        # permutation array\n",
    "        self.perm_array = np.array([])\n",
    "        \n",
    "    # function to get the next mini batch\n",
    "    def next_mini_batch(self):\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += self.mb_size\n",
    "        self.current_epoch += self.mb_size/len(self.x_train)  \n",
    "        \n",
    "        # adapt length of permutation array\n",
    "        if not len(self.perm_array) == len(self.x_train):\n",
    "            self.perm_array = np.arange(len(self.x_train))\n",
    "        \n",
    "        # shuffle once at the start of epoch\n",
    "        if start == 0:\n",
    "            np.random.shuffle(self.perm_array)\n",
    "\n",
    "        # at the end of the epoch\n",
    "        if self.index_in_epoch > self.x_train.shape[0]:\n",
    "            np.random.shuffle(self.perm_array) # shuffle data\n",
    "            start = 0 # start next epoch\n",
    "            self.index_in_epoch = self.mb_size # set index to mini batch size\n",
    "            \n",
    "            if self.train_on_augmented_data:\n",
    "                # use augmented data for the next epoch\n",
    "                self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n",
    "                self.y_train_aug = self.y_train\n",
    "                \n",
    "        end = self.index_in_epoch\n",
    "        \n",
    "        if self.train_on_augmented_data:\n",
    "            # use augmented data\n",
    "            x_tr = self.x_train_aug[self.perm_array[start:end]]\n",
    "            y_tr = self.y_train_aug[self.perm_array[start:end]]\n",
    "        else:\n",
    "            # use original data\n",
    "            x_tr = self.x_train[self.perm_array[start:end]]\n",
    "            y_tr = self.y_train[self.perm_array[start:end]]\n",
    "        \n",
    "        return x_tr, y_tr\n",
    "               \n",
    "    # generate new images via rotations, translations, zoom using keras\n",
    "    def generate_images(self, imgs):\n",
    "    \n",
    "        print('generate new set of images')\n",
    "        \n",
    "        # rotations, translations, zoom\n",
    "        image_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n",
    "            zoom_range = 0.1)\n",
    "\n",
    "        # get transformed images\n",
    "        imgs = image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n",
    "                                    batch_size=len(imgs), shuffle = False).next()    \n",
    "\n",
    "        return imgs[0]\n",
    "\n",
    "    # weight initialization\n",
    "    def weight_variable(self, shape, name = None):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    # bias initialization\n",
    "    def bias_variable(self, shape, name = None):\n",
    "        initial = tf.constant(0.1, shape=shape) #  positive bias\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    # 2D convolution\n",
    "    def conv2d(self, x, W, name = None):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n",
    "\n",
    "    # max pooling\n",
    "    def max_pool_2x2(self, x, name = None):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                              padding='SAME', name = name)\n",
    "\n",
    "    # attach summaries to a tensor for TensorBoard visualization\n",
    "    def summary_variable(self, var, var_name):\n",
    "        with tf.name_scope(var_name):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "    # function to create the graph\n",
    "    def create_graph(self):\n",
    "\n",
    "        # reset default graph\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # variables for input and output \n",
    "        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None,28,28,1], \n",
    "                                        name='x_data_tf')\n",
    "        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None,10], name='y_data_tf')\n",
    "\n",
    "        # 1.layer: convolution + max pooling\n",
    "        self.W_conv1_tf = self.weight_variable([self.s_f_conv1, self.s_f_conv1, 1,\n",
    "                                                self.n_f_conv1], \n",
    "                                               name = 'W_conv1_tf') # (5,5,1,32)\n",
    "        self.b_conv1_tf = self.bias_variable([self.n_f_conv1], name = 'b_conv1_tf') # (32)\n",
    "        self.h_conv1_tf = tf.nn.relu(self.conv2d(self.x_data_tf, \n",
    "                                                 self.W_conv1_tf) + self.b_conv1_tf, \n",
    "                                     name = 'h_conv1_tf') # (.,28,28,32)\n",
    "        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, \n",
    "                                            name = 'h_pool1_tf') # (.,14,14,32)\n",
    "\n",
    "        # 2.layer: convolution + max pooling\n",
    "        self.W_conv2_tf = self.weight_variable([self.s_f_conv2, self.s_f_conv2, \n",
    "                                                self.n_f_conv1, self.n_f_conv2], \n",
    "                                               name = 'W_conv2_tf')\n",
    "        self.b_conv2_tf = self.bias_variable([self.n_f_conv2], name = 'b_conv2_tf')\n",
    "        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, \n",
    "                                                 self.W_conv2_tf) + self.b_conv2_tf, \n",
    "                                     name ='h_conv2_tf') #(.,14,14,32)\n",
    "        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,7,7,32)\n",
    "\n",
    "        # 3.layer: convolution + max pooling\n",
    "        self.W_conv3_tf = self.weight_variable([self.s_f_conv3, self.s_f_conv3, \n",
    "                                                self.n_f_conv2, self.n_f_conv3], \n",
    "                                               name = 'W_conv3_tf')\n",
    "        self.b_conv3_tf = self.bias_variable([self.n_f_conv3], name = 'b_conv3_tf')\n",
    "        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, \n",
    "                                                 self.W_conv3_tf) + self.b_conv3_tf, \n",
    "                                     name = 'h_conv3_tf') #(.,7,7,32)\n",
    "        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, \n",
    "                                            name = 'h_pool3_tf') # (.,4,4,32)\n",
    "\n",
    "        # 4.layer: fully connected\n",
    "        self.W_fc1_tf = self.weight_variable([4*4*self.n_f_conv3,self.n_n_fc1], \n",
    "                                             name = 'W_fc1_tf') # (4*4*32, 1024)\n",
    "        self.b_fc1_tf = self.bias_variable([self.n_n_fc1], name = 'b_fc1_tf') # (1024)\n",
    "        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,4*4*self.n_f_conv3], \n",
    "                                          name = 'h_pool3_flat_tf') # (.,1024)\n",
    "        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, \n",
    "                                             self.W_fc1_tf) + self.b_fc1_tf, \n",
    "                                   name = 'h_fc1_tf') # (.,1024)\n",
    "      \n",
    "        # add dropout\n",
    "        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n",
    "        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, \n",
    "                                           name = 'h_fc1_drop_tf')\n",
    "\n",
    "        # 5.layer: fully connected\n",
    "        self.W_fc2_tf = self.weight_variable([self.n_n_fc1, 10], name = 'W_fc2_tf')\n",
    "        self.b_fc2_tf = self.bias_variable([10], name = 'b_fc2_tf')\n",
    "        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), \n",
    "                                self.b_fc2_tf, name = 'z_pred_tf')# => (.,10)\n",
    "\n",
    "        # cost function\n",
    "        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n",
    "     \n",
    "        # optimisation function\n",
    "        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n",
    "        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n",
    "            self.cross_entropy_tf, name = 'train_step_tf')\n",
    "\n",
    "        # predicted probabilities in one-hot encoding\n",
    "        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n",
    "        \n",
    "        # tensor of correct predictions\n",
    "        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n",
    "                                          tf.argmax(self.y_data_tf, 1),\n",
    "                                          name = 'y_pred_correct_tf')  \n",
    "        \n",
    "        # accuracy \n",
    "        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n",
    "                                         name = 'accuracy_tf')\n",
    "\n",
    "        # tensors to save intermediate accuracies and losses during training\n",
    "        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                         name='train_loss_tf', validate_shape = False)\n",
    "        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                         name='valid_loss_tf', validate_shape = False)\n",
    "        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='train_acc_tf', validate_shape = False)\n",
    "        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='valid_acc_tf', validate_shape = False)\n",
    "     \n",
    "        # number of weights and biases\n",
    "        num_weights = (self.s_f_conv1**2*self.n_f_conv1 \n",
    "                       + self.s_f_conv2**2*self.n_f_conv1*self.n_f_conv2 \n",
    "                       + self.s_f_conv3**2*self.n_f_conv2*self.n_f_conv3 \n",
    "                       + 4*4*self.n_f_conv3*self.n_n_fc1 + self.n_n_fc1*10)\n",
    "        num_biases = self.n_f_conv1 + self.n_f_conv2 + self.n_f_conv3 + self.n_n_fc1\n",
    "        print('num_weights =', num_weights)\n",
    "        print('num_biases =', num_biases)\n",
    "        \n",
    "        return None  \n",
    "    \n",
    "    def attach_summary(self, sess):\n",
    "        \n",
    "        # create summary tensors for tensorboard\n",
    "        self.use_tb_summary = True\n",
    "        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n",
    "        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n",
    "        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n",
    "        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n",
    "        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n",
    "        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n",
    "        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n",
    "        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n",
    "        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n",
    "        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n",
    "        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n",
    "        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n",
    "\n",
    "        # merge all summaries for tensorboard\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        # initialize summary writer \n",
    "        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filepath = os.path.join(os.getcwd(), 'logs', (self.nn_name+'_'+timestamp))\n",
    "        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n",
    "        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n",
    "\n",
    "    def attach_saver(self):\n",
    "        # initialize tensorflow saver\n",
    "        self.use_tf_saver = True\n",
    "        self.saver_tf = tf.train.Saver()\n",
    "\n",
    "    # function to train the graph\n",
    "    def train_graph(self, sess, x_train, y_train, x_valid, y_valid, n_epoch = 1, \n",
    "                    train_on_augmented_data = False):\n",
    "\n",
    "        # train on original or augmented data\n",
    "        self.train_on_augmented_data = train_on_augmented_data\n",
    "        \n",
    "        # training and validation data\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_valid = x_valid\n",
    "        self.y_valid = y_valid\n",
    "        \n",
    "        # use augmented data\n",
    "        if self.train_on_augmented_data:\n",
    "            print('generate new set of images')\n",
    "            self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n",
    "            self.y_train_aug = self.y_train\n",
    "        \n",
    "        # parameters\n",
    "        mb_per_epoch = self.x_train.shape[0]/self.mb_size\n",
    "        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n",
    "        \n",
    "        # start timer\n",
    "        start = datetime.datetime.now();\n",
    "        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n",
    "        print('learnrate = ',self.learn_rate,', n_epoch = ', n_epoch,\n",
    "              ', mb_size = ', self.mb_size)\n",
    "        # looping over mini batches\n",
    "        for i in range(int(n_epoch*mb_per_epoch)+1):\n",
    "\n",
    "            # adapt learn_rate\n",
    "            self.learn_rate_pos = int(self.current_epoch // self.learn_rate_step_size)\n",
    "            if not self.learn_rate == self.learn_rate_array[self.learn_rate_pos]:\n",
    "                self.learn_rate = self.learn_rate_array[self.learn_rate_pos]\n",
    "                print(datetime.datetime.now()-start,': set learn rate to %.6f'%self.learn_rate)\n",
    "            \n",
    "            # get new batch\n",
    "            x_batch, y_batch = self.next_mini_batch() \n",
    "\n",
    "            # run the graph\n",
    "            sess.run(self.train_step_tf, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                    self.y_data_tf: y_batch, \n",
    "                                                    self.keep_prob_tf: self.keep_prob, \n",
    "                                                    self.learn_rate_tf: self.learn_rate})\n",
    "             \n",
    "            \n",
    "            # store losses and accuracies\n",
    "            if i%int(self.log_step*mb_per_epoch) == 0 or i == int(n_epoch*mb_per_epoch):\n",
    "             \n",
    "                self.n_log_step += 1 # for logging the results\n",
    "                \n",
    "                feed_dict_train = {\n",
    "                    self.x_data_tf: self.x_train[self.perm_array[:len(self.x_valid)]], \n",
    "                    self.y_data_tf: self.y_train[self.perm_array[:len(self.y_valid)]], \n",
    "                    self.keep_prob_tf: 1.0}\n",
    "                \n",
    "                feed_dict_valid = {self.x_data_tf: self.x_valid, \n",
    "                                   self.y_data_tf: self.y_valid, \n",
    "                                   self.keep_prob_tf: 1.0}\n",
    "                \n",
    "                # summary for tensorboard\n",
    "                if self.use_tb_summary:\n",
    "                    train_summary = sess.run(self.merged, feed_dict = feed_dict_train)\n",
    "                    valid_summary = sess.run(self.merged, feed_dict = feed_dict_valid)\n",
    "                    self.train_writer.add_summary(train_summary, self.n_log_step)\n",
    "                    self.valid_writer.add_summary(valid_summary, self.n_log_step)\n",
    "                \n",
    "                train_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_train))\n",
    "\n",
    "                train_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_train))\n",
    "                \n",
    "                valid_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_valid))\n",
    "\n",
    "                valid_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_valid))\n",
    "\n",
    "                print('%.2f epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(\n",
    "                    self.current_epoch, train_loss[-1], valid_loss[-1],\n",
    "                    train_acc[-1], valid_acc[-1]))\n",
    "     \n",
    "        # concatenate losses and accuracies and assign to tensor variables\n",
    "        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n",
    "        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n",
    "        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n",
    "        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n",
    "   \n",
    "        sess.run(tf.assign(self.train_loss_tf, tl_c, validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_acc_tf, va_c , validate_shape = False))\n",
    "        \n",
    "        print('running time for training: ', datetime.datetime.now() - start)\n",
    "        return None\n",
    "  \n",
    "    # save tensors/summaries\n",
    "    def save_model(self, sess):\n",
    "        \n",
    "        # tf saver\n",
    "        if self.use_tf_saver:\n",
    "            #filepath = os.path.join(os.getcwd(), 'logs' , self.nn_name)\n",
    "            filepath = os.path.join(os.getcwd(), self.nn_name)\n",
    "            self.saver_tf.save(sess, filepath)\n",
    "        \n",
    "        # tb summary\n",
    "        if self.use_tb_summary:\n",
    "            self.train_writer.close()\n",
    "            self.valid_writer.close()\n",
    "        \n",
    "        return None\n",
    "  \n",
    "    # forward prediction of current graph\n",
    "    def forward(self, sess, x_data):\n",
    "        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n",
    "                                                 feed_dict = {self.x_data_tf: x_data,\n",
    "                                                              self.keep_prob_tf: 1.0})\n",
    "        return y_pred_proba\n",
    "    \n",
    "    # function to load tensors from a saved graph\n",
    "    def load_tensors(self, graph):\n",
    "        \n",
    "        # input tensors\n",
    "        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n",
    "        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n",
    "        \n",
    "        # weights and bias tensors\n",
    "        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n",
    "        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n",
    "        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n",
    "        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n",
    "        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n",
    "        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n",
    "        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n",
    "        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n",
    "        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n",
    "        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n",
    "        \n",
    "        # activation tensors\n",
    "        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n",
    "        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n",
    "        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n",
    "        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n",
    "        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n",
    "        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n",
    "        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        \n",
    "        # training and prediction tensors\n",
    "        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n",
    "        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n",
    "        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n",
    "        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n",
    "        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n",
    "        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n",
    "        \n",
    "        # tensor of stored losses and accuricies during training\n",
    "        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n",
    "        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n",
    "        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n",
    "        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n",
    "  \n",
    "        return None\n",
    "    \n",
    "    # get losses of training and validation sets\n",
    "    def get_loss(self, sess):\n",
    "        train_loss = self.train_loss_tf.eval(session = sess)\n",
    "        valid_loss = self.valid_loss_tf.eval(session = sess)\n",
    "        return train_loss, valid_loss \n",
    "        \n",
    "    # get accuracies of training and validation sets\n",
    "    def get_accuracy(self, sess):\n",
    "        train_acc = self.train_acc_tf.eval(session = sess)\n",
    "        valid_acc = self.valid_acc_tf.eval(session = sess)\n",
    "        return train_acc, valid_acc \n",
    "    \n",
    "    # get weights\n",
    "    def get_weights(self, sess):\n",
    "        W_conv1 = self.W_conv1_tf.eval(session = sess)\n",
    "        W_conv2 = self.W_conv2_tf.eval(session = sess)\n",
    "        W_conv3 = self.W_conv3_tf.eval(session = sess)\n",
    "        W_fc1_tf = self.W_fc1_tf.eval(session = sess)\n",
    "        W_fc2_tf = self.W_fc2_tf.eval(session = sess)\n",
    "        return W_conv1, W_conv2, W_conv3, W_fc1_tf, W_fc2_tf\n",
    "    \n",
    "    # get biases\n",
    "    def get_biases(self, sess):\n",
    "        b_conv1 = self.b_conv1_tf.eval(session = sess)\n",
    "        b_conv2 = self.b_conv2_tf.eval(session = sess)\n",
    "        b_conv3 = self.b_conv3_tf.eval(session = sess)\n",
    "        b_fc1_tf = self.b_fc1_tf.eval(session = sess)\n",
    "        b_fc2_tf = self.b_fc2_tf.eval(session = sess)\n",
    "        return b_conv1, b_conv2, b_conv3, b_fc1_tf, b_fc2_tf\n",
    "    \n",
    "    # load session from file, restore graph, and load tensors\n",
    "    def load_session_from_file(self, filename):\n",
    "        tf.reset_default_graph()\n",
    "        filepath = os.path.join(os.getcwd(), filename + '.meta')\n",
    "        #filepath = os.path.join(os.getcwd(),'logs', filename + '.meta')\n",
    "        saver = tf.train.import_meta_graph(filepath)\n",
    "        print(filepath)\n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, mn)\n",
    "        graph = tf.get_default_graph()\n",
    "        self.load_tensors(graph)\n",
    "        return sess\n",
    "    \n",
    "    # receive activations given the input\n",
    "    def get_activations(self, sess, x_data):\n",
    "        feed_dict = {self.x_data_tf: x_data, self.keep_prob_tf: 1.0}\n",
    "        h_conv1 = self.h_conv1_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_pool1 = self.h_pool1_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_conv2 = self.h_conv2_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_pool2 = self.h_pool2_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_conv3 = self.h_conv3_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_pool3 = self.h_pool3_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_fc1 = self.h_fc1_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        h_fc2 = self.z_pred_tf.eval(session = sess, feed_dict = feed_dict)\n",
    "        return h_conv1,h_pool1,h_conv2,h_pool2,h_conv3,h_pool3,h_fc1,h_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the neural network graph\n",
    "\n",
    "#nn_name = ['nn0','nn1','nn2','nn3','nn4','nn5','nn6','nn7','nn8','nn9']\n",
    "\n",
    "nn_name = ['tmp']\n",
    "\n",
    "# cross validations\n",
    "cv_num = 10 # cross validations default = 20 => 5% validation set\n",
    "kfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=123)\n",
    "\n",
    "for i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n",
    "    \n",
    "    # start timer\n",
    "    start = datetime.datetime.now();\n",
    "    \n",
    "    # train and validation data of original images\n",
    "    x_train = x_train_valid[train_index]\n",
    "    y_train = y_train_valid[train_index]\n",
    "    x_valid = x_train_valid[valid_index]\n",
    "    y_valid = y_train_valid[valid_index]\n",
    "    \n",
    "    # create neural network graph\n",
    "    nn_graph = nn_class(nn_name = nn_name[i]) # instance of nn_class\n",
    "    nn_graph.create_graph() # create graph\n",
    "    nn_graph.attach_saver() # attach saver tensors\n",
    "    \n",
    "    # start tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # attach summaries\n",
    "        nn_graph.attach_summary(sess) \n",
    "        \n",
    "        # variable initialization of the default graph\n",
    "        sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "        # training on original data\n",
    "        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 1.0)\n",
    "        \n",
    "        # training on augmented data\n",
    "        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 14.0,\n",
    "                            train_on_augmented_data = True)\n",
    "\n",
    "        # save tensors and summaries of model\n",
    "        nn_graph.save_model(sess)\n",
    "        \n",
    "    # only one iteration\n",
    "    if True:\n",
    "        break;\n",
    "        \n",
    "    \n",
    "print('total running time for training: ', datetime.datetime.now() - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization with tensorboard\n",
    "\n",
    "if False:\n",
    "    !tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confusion matrix\n",
    "\n",
    "mn = nn_name[0]\n",
    "nn_graph = nn_class()\n",
    "sess = nn_graph.load_session_from_file(mn)\n",
    "y_valid_pred[mn] = nn_graph.forward(sess, x_valid)\n",
    "sess.close()\n",
    "\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(\n",
    "    one_hot_to_dense(y_valid_pred[mn]), one_hot_to_dense(y_valid)).astype(np.float32)\n",
    "\n",
    "labels_array = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "fig, ax = plt.subplots(1,figsize=(10,10))\n",
    "ax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\n",
    "ax.set_xticklabels(labels_array)\n",
    "ax.set_yticklabels(labels_array)\n",
    "plt.title('Confusion matrix of validation set')\n",
    "plt.ylabel('True digit')\n",
    "plt.xlabel('Predicted digit')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss and accuracy curves\n",
    "\n",
    "mn = nn_name[0]\n",
    "nn_graph = nn_class()\n",
    "sess = nn_graph.load_session_from_file(mn)\n",
    "train_loss[mn], valid_loss[mn] = nn_graph.get_loss(sess)\n",
    "train_acc[mn], valid_acc[mn] = nn_graph.get_accuracy(sess)\n",
    "sess.close()\n",
    "\n",
    "print('final train/valid loss = %.4f/%.4f, train/valid accuracy = %.4f/%.4f'%(\n",
    "    train_loss[mn][-1], valid_loss[mn][-1], train_acc[mn][-1], valid_acc[mn][-1]))\n",
    "\n",
    "plt.figure(figsize=(10, 5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.plot(np.arange(0,len(train_acc[mn])), train_acc[mn],'-b', label='Training')\n",
    "plt.plot(np.arange(0,len(valid_acc[mn])), valid_acc[mn],'-g', label='Validation')\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.ylim(ymax = 1.1, ymin = 0.0)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('log steps');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0,len(train_loss[mn])), train_loss[mn],'-b', label='Training')\n",
    "plt.plot(np.arange(0,len(valid_loss[mn])), valid_loss[mn],'-g', label='Validation')\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.ylim(ymax = 3.0, ymin = 0.0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('log steps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## visualize weights\n",
    "\n",
    "mn = nn_name[0]\n",
    "nn_graph = nn_class()\n",
    "sess = nn_graph.load_session_from_file(mn)\n",
    "W_conv1, W_conv2, W_conv3, _, _ = nn_graph.get_weights(sess)\n",
    "sess.close()\n",
    "\n",
    "print('W_conv1: min = ' + str(np.min(W_conv1)) + ' max = ' + str(np.max(W_conv1))\n",
    "      + ' mean = ' + str(np.mean(W_conv1)) + ' std = ' + str(np.std(W_conv1)))\n",
    "print('W_conv2: min = ' + str(np.min(W_conv2)) + ' max = ' + str(np.max(W_conv2))\n",
    "      + ' mean = ' + str(np.mean(W_conv2)) + ' std = ' + str(np.std(W_conv2)))\n",
    "print('W_conv3: min = ' + str(np.min(W_conv3)) + ' max = ' + str(np.max(W_conv3))\n",
    "      + ' mean = ' + str(np.mean(W_conv3)) + ' std = ' + str(np.std(W_conv3)))\n",
    "\n",
    "s_f_conv1 = nn_graph.s_f_conv1\n",
    "s_f_conv2 = nn_graph.s_f_conv2\n",
    "s_f_conv3 = nn_graph.s_f_conv3\n",
    "\n",
    "W_conv1 = np.reshape(W_conv1,(s_f_conv1,s_f_conv1,1,6,6))\n",
    "W_conv1 = np.transpose(W_conv1,(3,0,4,1,2))\n",
    "W_conv1 = np.reshape(W_conv1,(s_f_conv1*6,s_f_conv1*6,1))\n",
    "\n",
    "W_conv2 = np.reshape(W_conv2,(s_f_conv2,s_f_conv2,6,6,36))\n",
    "W_conv2 = np.transpose(W_conv2,(2,0,3,1,4))\n",
    "W_conv2 = np.reshape(W_conv2,(6*s_f_conv2,6*s_f_conv2,6,6))\n",
    "W_conv2 = np.transpose(W_conv2,(2,0,3,1))\n",
    "W_conv2 = np.reshape(W_conv2,(6*6*s_f_conv2,6*6*s_f_conv2))\n",
    "\n",
    "W_conv3 = np.reshape(W_conv3,(s_f_conv3,s_f_conv3,6,6,36))\n",
    "W_conv3 = np.transpose(W_conv3,(2,0,3,1,4))\n",
    "W_conv3 = np.reshape(W_conv3,(6*s_f_conv3,6*s_f_conv3,6,6))\n",
    "W_conv3 = np.transpose(W_conv3,(2,0,3,1))\n",
    "W_conv3 = np.reshape(W_conv3,(6*6*s_f_conv3,6*6*s_f_conv3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize activations\n",
    "\n",
    "img_no = 10;\n",
    "mn = nn_name[0]\n",
    "nn_graph = nn_class()\n",
    "sess = nn_graph.load_session_from_file(mn)\n",
    "(h_conv1, h_pool1, h_conv2, h_pool2,h_conv3, h_pool3, h_fc1,\n",
    " h_fc2) = nn_graph.get_activations(sess, x_train_valid[img_no:img_no+1])\n",
    "sess.close()\n",
    "    \n",
    "# original image\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.subplot(2,4,1)\n",
    "plt.imshow(x_train_valid[img_no].reshape(28,28),cmap=cm.binary);\n",
    "\n",
    "# 1. convolution\n",
    "plt.subplot(2,4,2)\n",
    "plt.title('h_conv1 ' + str(h_conv1.shape))\n",
    "h_conv1 = np.reshape(h_conv1,(-1,28,28,6,6))\n",
    "h_conv1 = np.transpose(h_conv1,(0,3,1,4,2))\n",
    "h_conv1 = np.reshape(h_conv1,(-1,6*28,6*28))\n",
    "plt.imshow(h_conv1[0], cmap=cm.binary);\n",
    "\n",
    "# 1. max pooling\n",
    "plt.subplot(2,4,3)\n",
    "plt.title('h_pool1 ' + str(h_pool1.shape))\n",
    "h_pool1 = np.reshape(h_pool1,(-1,14,14,6,6))\n",
    "h_pool1 = np.transpose(h_pool1,(0,3,1,4,2))\n",
    "h_pool1 = np.reshape(h_pool1,(-1,6*14,6*14))\n",
    "plt.imshow(h_pool1[0], cmap=cm.binary);\n",
    "\n",
    "# 2. convolution\n",
    "plt.subplot(2,4,4)\n",
    "plt.title('h_conv2 ' + str(h_conv2.shape))\n",
    "h_conv2 = np.reshape(h_conv2,(-1,14,14,6,6))\n",
    "h_conv2 = np.transpose(h_conv2,(0,3,1,4,2))\n",
    "h_conv2 = np.reshape(h_conv2,(-1,6*14,6*14))\n",
    "plt.imshow(h_conv2[0], cmap=cm.binary);\n",
    "\n",
    "# 2. max pooling\n",
    "plt.subplot(2,4,5)\n",
    "plt.title('h_pool2 ' + str(h_pool2.shape))\n",
    "h_pool2 = np.reshape(h_pool2,(-1,7,7,6,6))\n",
    "h_pool2 = np.transpose(h_pool2,(0,3,1,4,2))\n",
    "h_pool2 = np.reshape(h_pool2,(-1,6*7,6*7))\n",
    "plt.imshow(h_pool2[0], cmap=cm.binary);\n",
    "\n",
    "# 3. convolution\n",
    "plt.subplot(2,4,6)\n",
    "plt.title('h_conv3 ' + str(h_conv3.shape))\n",
    "h_conv3 = np.reshape(h_conv3,(-1,7,7,6,6))\n",
    "h_conv3 = np.transpose(h_conv3,(0,3,1,4,2))\n",
    "h_conv3 = np.reshape(h_conv3,(-1,6*7,6*7))\n",
    "plt.imshow(h_conv3[0], cmap=cm.binary);\n",
    "\n",
    "# 3. max pooling\n",
    "plt.subplot(2,4,7)\n",
    "plt.title('h_pool2 ' + str(h_pool3.shape))\n",
    "h_pool3 = np.reshape(h_pool3,(-1,4,4,6,6))\n",
    "h_pool3 = np.transpose(h_pool3,(0,3,1,4,2))\n",
    "h_pool3 = np.reshape(h_pool3,(-1,6*4,6*4))\n",
    "plt.imshow(h_pool3[0], cmap=cm.binary);\n",
    "\n",
    "# 4. FC layer\n",
    "plt.subplot(2,4,8)\n",
    "plt.title('h_fc1 ' + str(h_fc1.shape))\n",
    "h_fc1 = np.reshape(h_fc1,(-1,24,24))\n",
    "plt.imshow(h_fc1[0], cmap=cm.binary);\n",
    "\n",
    "# 5. FC layer\n",
    "np.set_printoptions(precision=2)\n",
    "print('h_fc2 = ', h_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stacking of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read test data\n",
    "\n",
    "# read test data from CSV file \n",
    "\n",
    "if os.path.isfile('data/test.csv'):\n",
    "    test_df = pd.read_csv('data/test.csv') # on local environment\n",
    "    print('test.csv loaded: test_df{0}'.format(test_df.shape))\n",
    "else:\n",
    "    print('Error: test.csv not found')\n",
    "    \n",
    "# transforma and normalize test data\n",
    "x_test = test_df.iloc[:,0:].values.reshape(-1,28,28,1) # (28000,28,28,1) array\n",
    "x_test = x_test.astype(np.float)\n",
    "x_test = normalize_data(x_test)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "\n",
    "# for saving results\n",
    "y_test_pred = {}\n",
    "y_test_pred_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking of neural networks\n",
    "\n",
    "if False:\n",
    "    \n",
    "    take_models = ['nn0','nn1','nn2','nn3','nn4','nn5','nn6','nn7','nn8','nn9']\n",
    "\n",
    "    # cross validations\n",
    "    # choose the same seed as was done for training the neural nets\n",
    "    kfold = sklearn.model_selection.KFold(len(take_models), shuffle=True, random_state = 123)\n",
    "\n",
    "    # train and test data for meta model\n",
    "    x_train_meta = np.array([]).reshape(-1,10)\n",
    "    y_train_meta = np.array([]).reshape(-1,10)\n",
    "    x_test_meta = np.zeros((x_test.shape[0], 10))\n",
    "\n",
    "    print('Out-of-folds predictions:')\n",
    "\n",
    "    # make out-of-folds predictions from base models\n",
    "    for i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n",
    "\n",
    "        # training and validation data\n",
    "        x_train = x_train_valid[train_index]\n",
    "        y_train = y_train_valid[train_index]\n",
    "        x_valid = x_train_valid[valid_index]\n",
    "        y_valid = y_train_valid[valid_index]\n",
    "\n",
    "        # load neural network and make predictions\n",
    "        mn = take_models[i] \n",
    "        nn_graph = nn_class()\n",
    "        sess = nn_graph.load_session_from_file(mn)\n",
    "        y_train_pred[mn] = nn_graph.forward(sess, x_train[:len(x_valid)])\n",
    "        y_valid_pred[mn] = nn_graph.forward(sess, x_valid)\n",
    "        y_test_pred[mn] = nn_graph.forward(sess, x_test)\n",
    "        sess.close()\n",
    "\n",
    "        # collect train and test data for meta model \n",
    "        x_train_meta = np.concatenate([x_train_meta, y_valid_pred[mn]])\n",
    "        y_train_meta = np.concatenate([y_train_meta, y_valid]) \n",
    "        x_test_meta += y_test_pred[mn]\n",
    "\n",
    "        print(take_models[i],': train/valid accuracy = %.4f/%.4f'%(\n",
    "            accuracy_from_one_hot_labels(y_train_pred[mn], y_train[:len(x_valid)]),\n",
    "            accuracy_from_one_hot_labels(y_valid_pred[mn], y_valid)))\n",
    "\n",
    "        if False:\n",
    "            break;\n",
    "\n",
    "    # take average of test predictions\n",
    "    x_test_meta = x_test_meta/(i+1)\n",
    "    y_test_pred['stacked_models'] = x_test_meta\n",
    "\n",
    "    print('')\n",
    "    print('Stacked models: valid accuracy = %.4f'%accuracy_from_one_hot_labels(x_train_meta,\n",
    "                                                                               y_train_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use meta model\n",
    "\n",
    "if False:\n",
    "    \n",
    "    logreg = sklearn.linear_model.LogisticRegression(verbose=0, solver='lbfgs',\n",
    "                                                     multi_class='multinomial')\n",
    "    \n",
    "    # choose meta model\n",
    "    take_meta_model = 'logreg'\n",
    "\n",
    "    # train meta model\n",
    "    model = sklearn.base.clone(base_models[take_meta_model]) \n",
    "    model.fit(x_train_meta, one_hot_to_dense(y_train_meta))\n",
    "    \n",
    "    y_train_pred['meta_model'] = model.predict_proba(x_train_meta)\n",
    "    y_test_pred['meta_model'] = model.predict_proba(x_test_meta)\n",
    "\n",
    "    print('Meta model: train accuracy = %.4f'%accuracy_from_one_hot_labels(x_train_meta, \n",
    "                                                           y_train_pred['meta_model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose one single model for test prediction\n",
    "\n",
    "if True:\n",
    "    \n",
    "    mn = nn_name[0] # choose saved model\n",
    "    nn_graph = nn_class() # create instance\n",
    "    sess = nn_graph.load_session_from_file(mn) # receive session \n",
    "    y_test_pred = {}\n",
    "    y_test_pred_labels = {}\n",
    "\n",
    "    # split evaluation of test predictions into batches\n",
    "    kfold = sklearn.model_selection.KFold(40, shuffle=False) \n",
    "    for i,(train_index, valid_index) in enumerate(kfold.split(x_test)):\n",
    "        if i==0:\n",
    "            y_test_pred[mn] = nn_graph.forward(sess, x_test[valid_index])\n",
    "        else: \n",
    "            y_test_pred[mn] = np.concatenate([y_test_pred[mn],\n",
    "                                              nn_graph.forward(sess, x_test[valid_index])])\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the test predictions and submit the results\n",
    "\n",
    "#mn = 'meta_model'\n",
    "mn = nn_name[0]\n",
    "y_test_pred_labels[mn] = one_hot_to_dense(y_test_pred[mn])\n",
    "\n",
    "print(mn+': y_test_pred_labels[mn].shape = ', y_test_pred_labels[mn].shape)\n",
    "unique, counts = np.unique(y_test_pred_labels[mn], return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# save predictions\n",
    "np.savetxt('submission.csv', \n",
    "           np.c_[range(1,len(x_test)+1), y_test_pred_labels[mn]], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')\n",
    "\n",
    "print('submission.csv completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at some test images and predicted labels\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "for j in range(0,5):\n",
    "    for i in range(0,10):\n",
    "        plt.subplot(10,10,j*10+i+1)\n",
    "        plt.title('%d'%y_test_pred_labels[mn][j*10+i])\n",
    "        plt.imshow(x_test[j*10+i].reshape(28,28), cmap=cm.binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
